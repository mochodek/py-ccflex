#!/usr/bin/env python

# An independent script for manually selecting and labeling data

import argparse
import logging
import os
import pandas as pd
import numpy as np
import warnings

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB

from modAL.models import ActiveLearner

from common.configuration import ConfigurationHandler
from common.workspace import WorkspaceHandler

logger = logging.getLogger('pyccflex')
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
logger.addHandler(ch)

if __name__ == '__main__':

    logger.info("\n#### Running: {}".format(__file__))

    # Parse input parameters
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_files", help="The name of the input file containing features and contents",
                        nargs='+', type=str, required=True)
    parser.add_argument("--output_file", help="The name of the output file",
                        default=False, type=str)
    parser.add_argument("--base_learner", help="The name of classification algorithm used to evaluate",
                        default=False, type=str)
    parser.add_argument("--use_existing_labels",
                        help="The already added labels will be used; lines that are not explicitly "
                             "labeled will be treated as unknown",
                        default=False, action='store_true')
    parser.add_argument("--locations_config", help="Path to locations configuration file",
                        type=str, required=False, default="./locations.json")
    parser.add_argument("--files_format_config", help="Path to files format configuration file",
                        type=str, required=False, default="./files_format.json")
    parser.add_argument("--classes_config", help="Path to classes configuration file",
                        type=str, required=False, default="./classes.json")
    parser.add_argument("--classifiers_options", help="Path to classifiers options file",
                        type=str, required=False, default="./classifiers_options.json")
    parser.add_argument("--add_contents", help="Shall the content of the line be added to output?",
                        default=False, action='store_true')
    parser.add_argument("--max_lines", help="The maximum number of lines to read from each input file",
                        type=int, required=False, default=10 ** 5)

    args = vars(parser.parse_args())
    logger.info("Run parameters: {}".format(str(args)))

    locations_file_path = args['locations_config']
    files_format_file_path = args['files_format_config']
    classes_file_path = args['classes_config']
    classifiers_options_file_path = args['classifiers_options']

    input_files = args['input_files']
    output_file = args['output_file']
    base_learner = args['base_learner']
    use_existing_labels = args['use_existing_labels']
    max_lines = args['max_lines']

    try:
        locations_config = ConfigurationHandler(locations_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(locations_file_path))
        exit(1)

    try:
        files_format_config = ConfigurationHandler(files_format_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(files_format_file_path))
        exit(1)

    try:
        classifiers_options_config = ConfigurationHandler(classifiers_options_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(classifiers_options_file_path))
        exit(1)

    try:
        classes_config = ConfigurationHandler(classes_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(classes_file_path))
        exit(1)
    decision_classes = classes_config.get("classes", {})

    csv_separator = files_format_config.get("csv_sep", ",")

    workspace_dir_conf = locations_config.get('workspace_dir', None)
    workspace_dir_path = workspace_dir_conf.get("path", "")
    workspace_dir = WorkspaceHandler(workspace_dir_path)

    add_contents = args['add_contents']

    output_file_path = workspace_dir.get_processing_file_path(output_file)

    input_full_df = None

    for input_file in input_files:
        input_file_path = workspace_dir.get_processing_file_path(input_file)

        logger.info(">>> Loading file {}".format(input_file))

        input_df = pd.read_csv(input_file_path, sep=csv_separator, encoding="utf-8", nrows=max_lines)
        columns = list(input_df.columns)
        if 'contents' in columns:
            input_df.contents = input_df.contents.fillna("")
        else:
            input_df['contents'] = np.array([''] * input_df.shape[0])

        if 'class_value' not in columns:
            # input_df['class_value'] = np.array([decision_classes['default']['value']] * input_df.shape[0])
            # input_df['class_name'] = np.array([decision_classes['default']['name']] * input_df.shape[0])
            input_df['class_value'] = np.array([None] * input_df.shape[0])
            input_df['class_name'] = np.array([None] * input_df.shape[0])

        if input_full_df is None:
            input_full_df = input_df
        else:
            input_full_df = pd.concat([input_full_df, input_df], axis=0, ignore_index=True)

    if use_existing_labels:
        train_df = input_full_df[input_full_df['class_value'].notnull()]
        train_df.is_copy = False
        input_full_df = input_full_df[input_full_df['class_value'].isnull()]

    decision_class_name_column = None
    decision_class_value_column = None
    contents_column = None
    ids_column = input_full_df['id']
    file_names = [x.split(":")[0] for x in list(input_full_df['id'])]

    columns = list(input_full_df.columns)
    columns_to_drop = ['id']
    if decision_class_name_column is None and 'class_value' in columns:
        decision_class_name_column = input_full_df['class_name']
        decision_class_value_column = input_full_df['class_value']
        columns_to_drop.append('class_value')
        columns_to_drop.append('class_name')

    if contents_column is None and 'contents' in columns:
        contents_column = input_full_df['contents']
        columns_to_drop.append('contents')

    input_full_df.drop(columns_to_drop, inplace=True, axis=1)

    output_contents = []
    output_ids = []
    ids_column_list = list(ids_column)
    contents_list = list(contents_column)

    X_pool = input_full_df.values
    X_train = None
    y_train = None
    if use_existing_labels:
        y_train = train_df['class_value']
        output_ids = list(train_df['id'])
        output_contents = list(train_df['contents'])
        train_df.drop(columns_to_drop, inplace=True, axis=1)
        X_train = train_df.values

    model = None
    model_options = classifiers_options_config.get(base_learner, {})
    if base_learner == "CART":
        model = DecisionTreeClassifier(**model_options)

    elif base_learner == "RandomForest":
        model = RandomForestClassifier(**model_options)

    # active learner - default - uncertainty
    learner = ActiveLearner(
        estimator=model,
        X_training=X_train, y_training=y_train,
    )

    query_idx = list(np.random.choice(range(0, X_pool.shape[0]), 1, replace=False))

    prompt_message = ", ".join(["[{}]-{}".format(x['value'], x['name']) for x in decision_classes['labeled']])
    prompt_message = "{}, [enter]-{}, [q] to finish: ".format(prompt_message, decision_classes['default']['name'])

    class_values = [str(decision_classes['default']['value'])]
    class_values.extend([str(x['value']) for x in decision_classes['labeled']])

    cmd = None

    print("Please, label the following lines:")

    count_line = 1
    while True:
        query_idx = query_idx[0]
        file_name = file_names[query_idx]
        print("{}:".format(file_name))
        lines_to_display = [i for i in range(query_idx - 3, query_idx + 3) if
                            file_name == file_names[i] and i >= 0 and i < len(file_names)]

        for line_id in lines_to_display:
            if line_id == query_idx:
                spacer = ">>>"
            else:
                spacer = "   "
            print('{} {}  {}'.format(spacer, ids_column_list[line_id].split(":")[-1], contents_list[line_id]))
        cmd = input("{}# Choose: {}".format(count_line, prompt_message))
        print("\n")
        count_line += 1

        label = 0
        if cmd == "q":
            break
        if cmd in class_values:
            label = int(cmd)

        X_new = X_pool[query_idx].reshape(1, -1)
        y_new = np.array(label).reshape(1, )

        output_contents.append(contents_list[query_idx])
        output_ids.append(ids_column_list[query_idx])

        learner.teach(
            X=X_new,
            y=y_new,
        )
        # remove queried instance from pool
        X_pool = np.delete(X_pool, query_idx, axis=0)
        del contents_list[query_idx]
        del ids_column_list[query_idx]
        del file_names[query_idx]

        query_idx, query_sample = learner.query(X_pool)


        if cmd == "q":
            break

    print("Saving results to the file".format(output_file_path))

    output_x = learner.X_training
    output_y = learner.y_training

    class_values = np.array(output_y)

    class_value_to_name = dict((x['value'], x['name']) for x in decision_classes['labeled'])
    class_value_to_name[decision_classes['default']['value']] = decision_classes['default']['name']
    class_names = np.array([class_value_to_name[x] if x is not None else None for x in list(class_values)])

    output_df = pd.DataFrame(np.array(output_x), columns=list(input_full_df.columns))
    output_df = pd.concat(
        [pd.DataFrame(pd.Series(np.array(output_ids), name="id")), output_df, pd.DataFrame({"class_name": class_names}),
         pd.DataFrame({"class_value": class_values})],
        axis=1)

    if add_contents:
        output_df = pd.concat([output_df, pd.Series(np.array(output_contents), name='contents')], axis=1)

    output_df = output_df[output_df['class_value'].notnull()]

    if add_contents:
        line_numbers = output_df['id'].apply(lambda x: int(x.split(":")[-1]))
        files = output_df['id'].apply(lambda x: x.split(":")[0])

        lines_df = pd.concat(
            [output_df['id'], line_numbers, output_df['contents'], output_df[['class_name', 'class_value']], files],
            axis=1)
        lines_df.set_axis(labels=('id', 'line', 'contents', 'class_name', 'class_value', 'path'), axis=1, inplace=True)
        lines_output_file_path = workspace_dir.get_processing_file_path("lines-" + output_file)
        lines_df.to_csv(lines_output_file_path, sep=csv_separator, index=False, encoding="utf-8")

    output_df.to_csv(output_file_path, sep=csv_separator, index=False, encoding="utf-8")
