#!/usr/bin/env python

# Extracts basic manual features
import argparse
import logging
import os
import json
import time
import csv

from common.configuration import ConfigurationHandler
from common.workspace import WorkspaceHandler
from prepare.vocabularies import VocabularyExtractor, code_stop_words_tokenizer, token_signature
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

logger = logging.getLogger('pyccflex')
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
logger.addHandler(ch)

if __name__ == '__main__':

    logger.info("\n#### Running: {}".format(__file__))

    parser = argparse.ArgumentParser()
    parser.add_argument("lines_file",
                        help="CSV file with the extracted lines - either path or name of the file "
                             "that will be searched in the processing directory of the workspace",
                        type=str)
    parser.add_argument("vocabulary_file_name",
                        help="Name of the vocabulary file",
                        type=str)
    parser.add_argument("--locations_config", help="Path to locations configuration file",
                        type=str, required=False, default="./locations.json")
    parser.add_argument("--files_format_config", help="Path to files format configuration file",
                        type=str, required=False, default="./files_format.json")
    parser.add_argument("--include_statistics", help="Shall the result include frequency statistics",
                        default=False, action='store_true')
    parser.add_argument("--token_signature_for_missing", help="If token is not in vocabulary a signature will be used",
                        default=False, action='store_true')
    parser.add_argument("--top_words_threshold", help="How many most frequent words to take",
                        type=int, required=False, default=-1)
    parser.add_argument("--min_ngrams", help="Min. number of n-grams",
                        type=int, required=False, default=1)
    parser.add_argument("--max_ngrams", help="Max. number of n-grams",
                        type=int, required=False, default=1)
    parser.add_argument("--skip_generating_base_vocabulary", help="Assumes that the base vocabulary is available",
                        default=False, action='store_true')
    args = vars(parser.parse_args())
    logger.info("Run parameters: {}".format(str(args)))

    locations_file_path = args['locations_config']
    files_format_file_path = args['files_format_config']

    try:
        locations_config = ConfigurationHandler(locations_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(locations_file_path))
        exit(1)

    try:
        files_format_config = ConfigurationHandler(files_format_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(files_format_file_path))
        exit(1)
    separator = files_format_config.get("csv_sep", ",")

    workspace_dir_conf = locations_config.get('workspace_dir', None)
    workspace_dir_path = workspace_dir_conf.get("path", "")
    workspace_dir = WorkspaceHandler(workspace_dir_path)

    lines_file = args['lines_file']
    vocabulary_file_name = args['vocabulary_file_name']
    include_statistics = args['include_statistics']
    top_words_threshold = args['top_words_threshold']
    token_signature_for_missing = args['token_signature_for_missing']
    min_ngrams = args['min_ngrams']
    max_ngrams = args['max_ngrams']
    skip_generating_base_vocabulary = args['skip_generating_base_vocabulary']

    if os.path.exists(lines_file):
        input_file_path = lines_file
    else:
        input_file_path = workspace_dir.get_processing_file_path(lines_file)
    logger.info(">>> Extracting vocabulary from file {}".format(input_file_path))

    output_file_path = workspace_dir.get_processing_file_path(vocabulary_file_name)
    base_output_file_path = workspace_dir.get_processing_file_path("base-" + vocabulary_file_name)
    base_output_json_file_path = workspace_dir.get_processing_file_path(
        "base-" + vocabulary_file_name.replace(".csv", ".json"))

    if not skip_generating_base_vocabulary:
        logger.info(">>> Extracting base vocabulary")
        vocab_extractor = VocabularyExtractor(input_file_path, separator)
        vocab_extractor.extract()

        base_vocab = vocab_extractor.vocab
        if top_words_threshold > 0:
            logger.info(">>>> Taking {} most frequent words".format(top_words_threshold))
            base_vocab = base_vocab.head(top_words_threshold)

        if include_statistics:
            base_vocab.to_csv(base_output_file_path, sep=separator, index=False, encoding="utf-8",
                              quoting=csv.QUOTE_NONNUMERIC)
        else:
            base_vocab[['token']].to_csv(base_output_file_path, sep=separator, index=False, encoding="utf-8",
                                         quoting=csv.QUOTE_NONNUMERIC)
        logger.info(">>> Base vocabulary stored in the file {}".format(base_output_file_path))

        base_vocab_json = [{"name": x, "string": [x]} for x in list(base_vocab.token)]
        with open(base_output_json_file_path, 'w') as fp:
            json.dump(base_vocab_json, fp)
            logger.info(">>> Base vocabulary stored in the file {}".format(base_output_json_file_path))
    else:
        base_vocab = pd.read_csv(base_output_file_path, sep=separator, encoding="utf-8")

    logger.info(">>> Creating vocabulary including n-grams")

    internal_tokenizer = code_stop_words_tokenizer
    vocab_tokens = set(base_vocab.token)
    if token_signature_for_missing:

        def tokenize_with_signatures_for_missing(s):
            tokenized = internal_tokenizer(s)
            result = []
            for token in tokenized:
                if token in vocab_tokens:
                    result.append(token)
                else:
                    result.append(token_signature(token))
            return result


        logger.info(">>> Using a tokenizer that will use token signatures for tokens outside of the base vocabulary")
        tokenizer = tokenize_with_signatures_for_missing
    else:

        def tokenize_skipping_missing(s):
            tokenized = internal_tokenizer(s)
            result = []
            for token in tokenized:
                if token in vocab_tokens:
                    result.append(token)
            return result


        logger.info(">>> Using a tokenizer that will skip tokens outside of the base vocabulary")
        tokenizer = tokenize_skipping_missing

    start = time.process_time()

    count_vect = CountVectorizer(ngram_range=(min_ngrams, max_ngrams),
                                 tokenizer=tokenizer)
    lines_data = pd.read_csv(input_file_path, sep=separator, encoding="utf-8")
    lines_data.contents = lines_data.contents.fillna("")
    contents = list(lines_data.contents)
    count_vect.fit(contents)

    tokens = sorted(count_vect.vocabulary_.keys(), key=count_vect.vocabulary_.get)
    result_vocab = pd.DataFrame({"token": tokens})

    result_vocab.to_csv(output_file_path, sep=separator, index=False, encoding="utf-8", quoting=csv.QUOTE_NONNUMERIC)

    end = time.process_time()
    print(end - start)

    logger.info(">>> Vocabulary saved to {}".format(output_file_path))
