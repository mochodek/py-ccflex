#!/usr/bin/env python

# Extracts basic manual features
import argparse
import logging

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

from common.configuration import ConfigurationHandler
from common.workspace import WorkspaceHandler
from prepare.feature_extractors import LineFeaturesExtractionController, CountVectorizerBasedFeatureExtraction
from prepare.vocabularies import code_stop_words_tokenizer, token_signature

logger = logging.getLogger('pyccflex')
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
logger.addHandler(ch)



if __name__ == '__main__':

    logger.info("\n#### Running: {}".format(__file__))

    parser = argparse.ArgumentParser()
    parser.add_argument("code_location_config_key",
                        help="Name of the node in configuration defining path to code", type=str)
    parser.add_argument("vocabulary_file_name",
                        help="Name of the vocabulary file", type=str)
    parser.add_argument("--locations_config", help="Path to locations configuration file",
                        type=str, required=False, default="./locations.json")
    parser.add_argument("--files_format_config", help="Path to files format configuration file",
                        type=str, required=False, default="./files_format.json")
    parser.add_argument("--add_decision_class", help="Shall the decision class be added to output?",
                        default=False, action='store_true')
    parser.add_argument("--add_contents", help="Shall the content of the line be added to output?",
                        default=False, action='store_true')
    parser.add_argument("--token_signature_for_missing", help="Shall the content of the line be added to output?",
                        default=False, action='store_true')
    parser.add_argument("--min_ngrams", help="Min. number of n-grams",
                        type=int, required=False, default=1)
    parser.add_argument("--max_ngrams", help="Max. number of n-grams",
                        type=int, required=False, default=1)
    parser.add_argument("--chunk_size", help="Number of lines to process in a batch",
                        type=int, required=False, default=10 ** 3)
    parser.add_argument("--max_line_length", help="Maximum line length; if exceeded it will be truncated.",
                        type=int, required=False, default=1000)

    args = vars(parser.parse_args())
    logger.info("Run parameters: {}".format(str(args)))

    locations_file_path = args['locations_config']
    files_format_file_path = args['files_format_config']
    chunk_size = args['chunk_size']
    max_line_length = args['max_line_length']

    try:
        locations_config = ConfigurationHandler(locations_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(locations_file_path))
        exit(1)

    try:
        files_format_config = ConfigurationHandler(files_format_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(files_format_file_path))
        exit(1)
    separator = files_format_config.get("csv_sep", ",")

    workspace_dir_conf = locations_config.get('workspace_dir', None)
    workspace_dir_path = workspace_dir_conf.get("path", "")
    workspace_dir = WorkspaceHandler(workspace_dir_path)

    code_loc_config_key = args['code_location_config_key']
    code_loc = locations_config.get(code_loc_config_key, None)
    input_file_path = workspace_dir.get_processing_file_path("{}-lines.csv".format(code_loc_config_key))
    output_file_path = workspace_dir.get_processing_file_path("{}-bag-of-words.csv".format(code_loc_config_key))

    vocabulary_file_path = workspace_dir.get_processing_file_path(args['vocabulary_file_name'])
    base_vocabulary_file_path = workspace_dir.get_processing_file_path("base-" + args['vocabulary_file_name'])

    add_decision_class = args['add_decision_class']
    add_contents = args['add_contents']

    token_signature_for_missing = args['token_signature_for_missing']
    min_ngrams = args['min_ngrams']
    max_ngrams = args['max_ngrams']

    logger.info(">>> Loading vocabularies")

    base_vocab = pd.read_csv(base_vocabulary_file_path, sep=separator, encoding="utf-8")
    vocabulary = pd.read_csv(vocabulary_file_path, sep=separator, encoding="utf-8")

    internal_tokenizer = code_stop_words_tokenizer
    vocab_tokens = set(base_vocab.token)

    if token_signature_for_missing:

        def tokenize_with_signatures_for_missing(s):
            tokenized = internal_tokenizer(s)
            result = []
            for token in tokenized:
                if token in vocab_tokens:
                    result.append(token)
                else:
                    result.append(token_signature(token))
            return result


        logger.info(">>> Using a tokenizer that will use token signatures for tokens outside of the base vocabulary")
        tokenizer = tokenize_with_signatures_for_missing
    else:

        def tokenize_skipping_missing(s):
            tokenized = internal_tokenizer(s)
            result = []
            for token in tokenized:
                if token in vocab_tokens:
                    result.append(token)
            return result


        logger.info(">>> Using a tokenizer that will skip tokens outside of the base vocabulary")
        tokenizer = tokenize_skipping_missing

    logger.info(">>> Extracting features")

    tokens = list(vocabulary['token'])
    vocab = dict((key, value) for value, key in enumerate(tokens))

    count_vect = CountVectorizer(ngram_range=(min_ngrams, max_ngrams), tokenizer=tokenizer, vocabulary=vocab, lowercase=False)

    lines_data_chunks = pd.read_csv(input_file_path, sep=separator, encoding="utf-8", chunksize=chunk_size)

    for lines_data in lines_data_chunks:
        lines_data.contents = lines_data.contents.fillna("")
        contents = [line if len(line) < max_line_length else line[:max_line_length] for line in lines_data.contents]
        count_vect.fit_transform(contents)
        break

    extractors = [CountVectorizerBasedFeatureExtraction(count_vect, separator, "{}".format(separator))]

    controller = LineFeaturesExtractionController(extractors,
                                                  input_file_path, output_file_path,
                                                  sep=separator,
                                                  add_decision_class=add_decision_class,
                                                  add_contents=add_contents)
    controller.extract()

    logger.info(">>> Bag of words features saved to file {}".format(output_file_path))

