#!/usr/bin/env python

# Adds features from +-n lines / cases (all have to be in the same file)

import argparse
import logging
import pandas as pd
import csv
import gc

from common.configuration import ConfigurationHandler
from common.workspace import WorkspaceHandler

logger = logging.getLogger('pyccflex')
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
logger.addHandler(ch)

if __name__ == '__main__':

    logger.info("\n#### Running: {}".format(__file__))

    # Parse input parameters
    parser = argparse.ArgumentParser()
    parser.add_argument("input_file",
                        help="Path to input csv file", type=str)
    parser.add_argument("output_file",
                        help="Path to output csv file", type=str)
    parser.add_argument("--prev_cases", help="The number of preceding cases",
                        default=1, type=int)
    parser.add_argument("--next_cases", help="The number of proceeding cases",
                        default=1, type=int)
    parser.add_argument("--locations_config", help="Path to locations configuration file",
                        type=str, required=False, default="./locations.json")
    parser.add_argument("--files_format_config", help="Path to files format configuration file",
                        type=str, required=False, default="./files_format.json")
    parser.add_argument("--add_decision_class", help="Shall the decision class be added to output?",
                        default=False, action='store_true')
    parser.add_argument("--add_contents", help="Shall the content of the line be added to output?",
                        default=False, action='store_true')
    parser.add_argument("--chunk_size", help="Number of lines to process in a batch",
                        type=int, required=False, default=10 ** 5)

    args = vars(parser.parse_args())
    logger.info("Run parameters: {}".format(str(args)))

    locations_file_path = args['locations_config']
    files_format_file_path = args['files_format_config']

    input_file = args['input_file']
    output_file = args['output_file']
    prev_cases = args['prev_cases']
    next_cases = args['next_cases']
    add_decision_class = args['add_decision_class']
    add_contents = args['add_contents']
    chunk_size = args['chunk_size']

    try:
        locations_config = ConfigurationHandler(locations_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(locations_file_path))
        exit(1)

    try:
        files_format_config = ConfigurationHandler(files_format_file_path)
    except Exception as e:
        logger.error("Couldn't load configuration file {}".format(files_format_file_path))
        exit(1)

    csv_separator = files_format_config.get("csv_sep", ",")

    workspace_dir_conf = locations_config.get('workspace_dir', None)
    workspace_dir_path = workspace_dir_conf.get("path", "")
    workspace_dir = WorkspaceHandler(workspace_dir_path)

    logger.info(">>> Loading input the file {}".format(input_file))

    input_file_path = workspace_dir.get_processing_file_path(input_file)
    input_df_chunks = pd.read_csv(input_file_path, sep=csv_separator, encoding="utf-8", chunksize=chunk_size)

    output_file_path = workspace_dir.get_processing_file_path(output_file)

    with open(output_file_path, "w", newline='', encoding="utf-8") as f:
        writer = csv.writer(f, delimiter=csv_separator, quotechar='"', quoting=csv.QUOTE_NONNUMERIC)

        for chunk_id, input_df in enumerate(input_df_chunks):

            logger.info(">>> Saving {} lines to the file {}".format(input_df.shape[0], output_file))

            # temporary remove class and contents
            decision_class_name_column = None
            decision_class_value_column = None
            contents_column = None
            ids_column = input_df['id']
            file_names = [x.split(":")[0] for x in list(ids_column)]
            columns = list(input_df.columns)
            collumns_to_drop = ['id']
            if 'class_value' in columns:
                decision_class_name_column = list(input_df['class_name'])
                decision_class_value_column = list(input_df['class_value'])
                collumns_to_drop.append('class_value')
                collumns_to_drop.append('class_name')
            if 'contents' in columns:
                contents_column = input_df['contents']
                collumns_to_drop.append('contents')
            input_df = input_df.drop(collumns_to_drop, inplace=False, axis=1)

            # preapare header
            if chunk_id == 0:
                header = ['id']
                for i in range(1, prev_cases + 1):
                    cols_prev = ["{}_prev{}".format(x, i) for x in list(input_df.columns)]
                    header.extend(cols_prev)

                for i in range(1, next_cases + 1):
                    cols_next = ["{}_next{}".format(x, i) for x in list(input_df.columns)]
                    header.extend(cols_next)

                header.extend(list(input_df.columns))

                if add_decision_class and decision_class_name_column is not None:
                    header.extend(['class_name', 'class_value'])

                if add_contents and contents_column is not None:
                    header.append('contents')

                writer.writerow(header)

            start = input_df.index.min()
            end = input_df.index.max()
            for i in input_df.index.values:
                row = [ids_column[i]]
                file_name = file_names[i - start]

                for j in range(1, prev_cases + 1):
                    if i - j >= start and file_names[i - j - start] == file_name:
                        row.extend(list(input_df.loc[i - j, :]))
                    else:
                        row.extend([0] * input_df.shape[1])

                for j in range(1, next_cases + 1):
                    if i + j <= end and file_names[i + j - start] == file_name:
                        row.extend(list(input_df.loc[i + j, :]))
                    else:
                        row.extend([0] * input_df.shape[1])

                row.extend(list(input_df.loc[i, :]))

                if add_decision_class and decision_class_name_column is not None:
                    row.append(decision_class_name_column[i])
                    row.append(decision_class_value_column[i])

                if add_contents and contents_column is not None:
                    row.append(contents_column[i])

                writer.writerow(row)

            del input_df
            gc.collect()

    logger.info(">>> Output saved to the file {}".format(output_file))